구분,내용
낯선 라이선스 약관,"현재 오픈소스 AI 라이선스 유형은 다양한 만큼 매우 복잡하다. 오픈소스 AI 모델을 이용하려면 상업적으로 사용해도 괜찮은지, 수정 및 배포가 가능한지, 사내 코드 베이스에 안전하게 통합할 수 있는지 등을 알아봐야 한다. 여기에 몇 가지 새로운 문제가 등장했다. 일단 이전에는 볼 수 없는 제약이 오픈소스 라이선스에 적용됐다.
메타의 라마(Llama) 라이선스를 예로 보자. 라마는 높은 인기를 자랑하는 오픈소스 LLM이다. 메타는 라마에 대해 ‘개방형 접근권을 제공하고 잠재적인 오용 문제를 해결하기 맞춤형 상용 라이선스 모델’을 적용한다고 밝혔다. 또한 이에 대해 ‘책임 및 보호 조치의 균형을 맞추기 위한 조치’라고 소개하고 있다.
이런 라이선스 하에 기업은 상업적으로 라마 모델을 사용할 수 있고 개발자는 기본 라마 모델 위에 추가 작업을 만들어 배포할 수 있다. 단 다른 LLM을 개선하기 위해 라마가 출력하는 결과물을 활용할 수는 없다. (라마 파생 모델은 제외된다.) 또한 기업 또는 그 계열사의 월간 사용자가 700명을 초과하는 경우, 메타에게 라이선스 사용 허락을 요청해야 한다. 메타는 이를 승인하거나 승인하지 않을 수 있다. 어떤 기업에서 라마 3를 사용해서 뭔가를 만들었다면, 눈에 잘 띄는 위치에 ‘Built with Llama 3(라마 3를 기반으로 구축했음)’라는 문구를 표기해야 한다."
낯선 라이선스 약관,"메타와 유사하게 애플은 ‘애플 샘플 코드 라이선스’하에 오픈ELM을 출시했다. 이 라이선스 역시 특허권은 제외하고 저작권 권한만 명시하고 있다. 애플이나 메타 모두 일반적으로 통용되는 오픈소스 라이선스를 사용하지는 않지만, 실제로 코드는 공개되어 있다. 여기에 애플은 실제로 코드뿐만 아니라 모델 가중치, 훈련 데이터 세트, 훈련 로그, 사전 훈련 구성도 공개했다.
이제 오픈소스 라이선스의 또 다른 측면을 살펴보자. 기존의 오픈소스 소프트웨어에서 핵심은 ‘코드’였다. 그리고 그 코드를 통해 소프트웨어가 어떤 기능을 하는지, 잠재적인 문제나 취약점이 있는지 확인할 수 있었다.
하지만 생성형 AI는 단순한 코드가 아니다. 학습 데이터, 모델 가중치, 미세 조정도 마찬가지다. 이러한 모든 요소는 모델의 작동 방식을 이해하고 잠재적인 편향을 파악하는 데 매우 중요다. 예를 들어, 평평한 지구 음모론에 대한 자료를 학습한 모델은 과학 관련 질문에 제대로 답하지 못할 수 있다. 북한 해커가 미세 조정한 모델은 멀웨어를 정확하게 식별하지 못할 수 있다. 그렇다면 오픈소스 LLM은 이러한 모든 정보를 공개할까? 모델에 따라 다르다. 심지어 버전에 따라 다를 수도 있다. 그만큼 관련된 업계 표준이 없다.
카네기멜론 대학교의 AI 교수이자 전 PwC의 글로벌 AI 사업 총괄이었 아난드 라오는 “AI 모델에서 코드를 공개하는 경우도 있지만, 미세 조정 관련 정보가 없다면 비슷한 성능을 얻기 위해 많은 비용을 지출할 수 있다”라고 설명했다."
전문성 부족,"오픈소스는 스스로 해야 하는 작업이 많을 수 있다. 기업은 코드를 다운로드할 수는 있지만, 제대로 작동시키려면 사내 전문가나 외부 컨설턴트가 필요할 수 있다. 이는 생성형 AI 분야에서 큰 문제다. 이제 막 나온 기술과 관련해서 수년간의 경험을 가진 사람은 존재할 수 없다. 라오는 그런면에서 생성형 AI를 완전 처음 시작하거나 빠르게 도입하고 싶은 기업이라면 차라리 상용 플랫폼으로 시작하는 것이 더 안전하다고 조언했다.
라오는 “오픈소스 버전을 다운로드하려면 전문 지식이 필요하다”라며 “개념 증명을 완료하고 모델을 실제 제품에 배포 후 비용이 늘어나고 있다면, 오픈소스의 대안을 본격적으로 살펴봐야 할 것”이라고 설명했다."
전문성 부족,"업계 전체에 전문성이 부족하다는 점은 새로운 문제를 야기한다. 원래 오픈소스의 장점 중 하나는 수많은 사람이 코드를 살펴보고 프로그래밍 오류, 보안 취약점 및 기타 약점을 발견할 수 있다는 부분이다. 마치 외부의 ‘천개의 눈’으로 문제점을 공동으로 확인하는 셈이다. 기존의 오픈소스 소프트웨어야 수많은 전문가가 있어 천개의 눈을 가질 수 있었지만 AI 분야에서는 전문가가 부족하다. 천개의 눈을 아직 모으지 못했으니 보안 취약점에 제대로 대응하지 못할 수도 있다."
탈옥,"LLM 공격 중 유명한 것에 탈옥(jailbreak)이라는 수법이 있다. 탈옥은 의도적으로 사용자가 제시한 지침을 어기게 하고 멀웨어를 생성하도록 속이는 교묘한 프롬프트를 만드는 것이다. 일부 벤더는 탈옥 현상을 파악해서 알려주고 막는 서비스를 제공하고 있다. 오픈소스 모델에 보내는 메시지에 접근하며 의심스러운 활동의 징후를 모니터링하는 벤더도 있다.
악의적인 공격자가 비공개 환경에서 실행되는 엔터프라이즈 버전의 제품을 구매할 가능성은 낮다. 애초에 프롬프트 정보가 기업 내부에서만 입력되고 벤더에게 전송되지 않기 때문이다."
탈옥,"반면에 악의적인 공격자는 접근하기 쉬운 오픈소스 모델을 무료로 다운로드하여 자신의 환경에서 실행하며 해킹을 시도할 수 있다. 또한 모델이 사용하는 시스템 프롬프트와 안전 기능까지 다 볼 수 있기 때문에 탈옥을 위한 유리한 고지를 선점할 수 있다. 기업에선 오픈소스 AI 모델 전문 보안 담당자가 없는 경우가 많기 때문에 이런 공격에 취약할 수 있다. 라오는 “예를 들어 공격자는 학습 데이터를 분석하여 모델이 이미지를 잘못 식별하거나 사용자가 인식하지 못하게 문제 없어 보이는 프롬프트를 만드는 방법을 알아낼 수 있다”라고 설명했다.
AI 모델 출력물에 워터마크를 추가해도 공격을 막지 못할 수도 있다. 악의적인 공격자가 코드를 분석 및 리버스 엔지니어링을 통해 워터마크를 제거할 수 있기 때문이다. 또한 공격자는 모델이나 기타 지원 코드 및 도구를 분석하여 취약한 영역을 찾을 수도 있다.
컨설팅 업체인 노탈(Nortal)의 수석 데이터 과학자인 엘레나 수기스는 “공격자는 특정 요청을 계속 보내 인프라를 과부하시켜 모델이 작동하지 않도록 만들 수 있다”라며 “모델이 특정 시스템의 일부이고 모델 출력 결과가 시스템의 다른 부분에서 사용되는 경우 문제가 될 수 있다. 특히 모델의 출력 생성 방식을 공격할 수 있다면 전체 시스템이 중단될 수 있다. 기업에 매우 위험한 일이다”라고 말했다."
학습 데이터가 가진 위험,"예술가, 작가, 기타 저작권 소유자들이 대형 AI 기업을 상대로 소송을 제기하고 있다. 하지만 오픈소스 모델이 지적재산권을 침해하고, 그 모델을 제품이나 서비스에 도입한 기업들만 큰돈을 벌고 있다면 어떻게 될까? 기업 사용자가 소송을 당할 수도 있을까?
EY의 구아레라는 “잠재적인 위험성은 있다. 법원에서 진행 중인 관련 소송이 어떤 결과를 맞이할지는 아직 아무도 모른다”라며 “데이터세트에 대한 보상이 있어야 하는 사회로 향하고 있을지도 모른다”라고 분석했다. 또한 구아레라는 “대형 기술 업체는 앞으로 늘어날 저작권 관련 위기를 극복하고 필요한 자금을 확보할 수 있는 더 나은 위치에 있다”라고 평가했다."
학습 데이터가 가진 위험,"노탈의 수기스는 “대형 벤더는 학습 데이터를 구입하고 소송을 벌이는 데 쓸 돈이 있을 뿐만 아니라, 선별된 데이터 세트에 쓸 돈도 있다”라고 설명했다. 무료로 공개된 데이터 세트에는 합법적인 콘텐츠만 있는 것이 아니다. 일부는 저작권이 있는데 허가 없이 포함된 상태일 수 있다. 거기다 부정확하고 편향된 정보, 멀웨어 및 출력 품질을 저하시킬 수 있는 기타 자료도 들어가 있다.
수기스는 “많은 모델 개발자가 큐레이션된 데이터를 사용하자고 이야기하고 있다”라며 “하지만 그렇게 하려면 인터넷 전체를 학습시키는 것보다 비용이 더 많이 든다”라고 밝혔다. "
새로운 보안 위협,"생성형 AI 프로젝트는 단순한 코드 그 이상이기 때문에 잠재적으로 보안 위협에 노출될 수 있는 영역이 더 많다. LLM은 여러 방면에서 악의적인 공격을 받을 수 있다. 수기스에 따르면, 이들은 관리가 제대로 이루어지지 않는 프로젝트의 개발팀에 침투하여 소프트웨어에 바로 악성 코드를 추가할 수 있다. 거기서 끝나지 않고 훈련 데이터, 미세 조정 또는 가중치까지 오염시킬 수 있다.
수기스는 “해커는 예시 악성 코드로 모델을 재학습시켜 사용자 인프라에 침입할 수 있다”라며 “또는 가짜 뉴스와 잘못된 정보로 모델을 훈련시킬 수도 있다”라고 설명했다."
새로운 보안 위협,또 다른 공격 벡터는 모델의 시스템 프롬프트이다. 수기스에 따르면시스템 프롬프트는 일반 사용자에게 보통 공개되지 않는다. 시스템 프롬프트는 모델이 원치 않거나 비윤리적인 행동을 인식하는 안전망을 포함하고 있기에 중요하다. 수기스는 “해커가 시스템 프롬프트에 접근해서 모델을 공격하는 방법을 알아낼 수 있다”라고 설명했다.
누락된 가드레일,"오픈소스 그룹 일각에서는 AI 모델에 가드레일(허용 가능한 범위를 두는 일종의 가이드라인 또는 도구)을 두는 것 자체를 반대하기도 한다. 모델에 아무런 제한 없어야 더 나은 성능을 발휘할 것이라 믿는 곳도 있다. 기업은 현재 사용하는 오픈소스 모델이 가드레일에 대해 어떤 방향을 추구하는지조차 잘 모를 수도 있다.
수기스는 “현재 오픈소스 생성형 AI 모델의 안전성을 평가하는 독립적인 기관은 없다”라며 “유럽의 AI 법은 이러한 문서를 일부 요구할 것이지만, 대부분의 조항은 2026년에야 시행될 것”이라고 밝혔다. 또한 수기스는 “나라면 가능한 한 많은 문서를 확보하고, 모델을 테스트 및 평가하고, 회사 내부에 몇 가지 보호 장치를 마련할 것”이라고 밝혔다."
표준 부족,"오픈소스 기술을 소비하는 기업이 표준과 호환성을 중시하는 경우가 많다. 그래서 사용자 주도의 오픈소스 프로젝트는 보통 기존에 나온 표준을 따를 때가 많다.
실제로 리눅스 재단이 약 500명의 기술 전문가를 대상으로 진행한 작년 설문조사에 따르면, 71%가 개방형 표준을 선호하는 반면, 폐쇄형 표준을 선호하는 응답자는 10%에 불과했다. 보통 상용 소프트웨어 업체는 고객이 자사 기술 생태계에 갇혀 있는 것을 선호한다. 오픈소스 기술은 다를 것이라고 기대할 수 있지만, 표준을 따르지 않는 생성형 AI 기술은 꽤 있다."
표준 부족,"사실 많은 사람이 AI 표준에 대해 이야기할 때 윤리, 개인정보 보호, 설명 가능성 등을 이야기한다. 실제로 작년 12월에 발표된 ISO/IEC 42001 표준과 같은 작업이 해당 영역을 다루고 있다. 그리고 지난 4월 미국 국립표준기술연구소(NIST)는 AI 관련 공통 언어를 제시하는 등 AI 표준 계획 초안을 발표했다. 여기에선 주로 위험과 거버넌스 문제에 초점을 맞추고 있다. 그럼에도 기술 표준에 관해서는 아직 정해진 것이 많지 않다.
클라우드 네이티브 컴퓨팅 재단의 CIO인 테일러 돌잘은 “표준에 대한 논의는 매우 초기 단계에 머물러 있다”라며 ”단 데이터 분류, 학습 데이터, API, 프롬프트에 대한 표준 형식에 대해 의미 있는 논의가 이뤄지고 있다”라고 말했다. 그래도 아직 ‘논의’하는 수준일 뿐이다."
